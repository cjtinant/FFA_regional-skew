---
title: "Milestone 01 â€” Download and Prepare Covariates"
subtitle: "Structured Acquisition and QA of Inputs for Regional Skew Estimation"
author: "C.J. Tinant"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 2
params:
  version: "v0.5"
editor_options:
  markdown:
    wrap: 72
---

## Overview of `r params$version`

This document outlines the setup, documentation, and reproducibility scaffolding established for **Milestone `r params$version`**, focused on acquiring and validating spatial covariates for regional skew modeling.

### Summary
Initiate acquisition, validation, and preparation of climate, terrain, and location-based covariates for use in regional skew estimation models.

See [`spatial_data_preparation_checklist.md`](../docs/spatial_data_preparation_checklist.md) for a reusable checklist used to guide and document preprocessing steps across spatial layers.

## Goals

**Update the covariate source inventory**

- Downloading and staging spatial datasets (ecoregions, PRISM, NED, etc.)
- Documenting file sources and formats in `metadata/` and `docs/`
- Conducting initial QA checks on file completeness and coordinate reference systems

-   Acquire, check, and document all raw covariate datasets

-   Write or Refactor Download Scripts

-   Spatial File QA + Metadata Logging

-   Version Control & Tagging

## Notes

-   This milestone builds on `v0.3-structure-refactor`

-   README-style documentation will be embedded in this .Rmd file for reproducibility

## Standardized Script Naming Conventions
To improve clarity and reproducibility, scripts follow a standardized naming format.

See: `R/00_setup/` and `data/raw/` for implementation scripts and acquired data.

### Subdirectory Naming Convention:
Subdirectories are named to reflect the `workflow stage`, `data source`, and `data domain or content category`, ensuring a transparent and reproducible project structure.

Subdirectory naming follows the format: `[stage]/[source]_[category]/`

**Where:**

-   `[stage]` â€“ Workflow status or type (e.g., `raw`, `processed`, `meta`, `interim`)

-   `[source]` â€“ Data provider or system (e.g., `epa`, `prism`, `usgs`, `ned`, `nlcd`)

-   `[category]` â€“ Broad data content or theme (e.g., `ecoregions`, `30yrnormals`, `landcover`,  `elev`, `catchments`

### Example Subdirectory Names

| Folder Path                       | Description |
| --------------------------------- | ----------- |
| `data/meta/epa_ecoregions/`       | Metadata or schema for EPA ecoregions shapefiles |

| `data/raw/prism_30yrnormals/`     | Raw PRISM precipitation normals |
| `data/processed/usgs_catchments/` | Processed USGS NHDPlus catchments |

| `data/interim/ned_elev/`          | Intermediate elevation surfaces (clipped or reprojected) |

### Script Naming Convention
Scripts follow a standardized naming format to promote readability, automation, and chronological sequencing within the workflow.

`[step#]_[task]_[source].R or .Rmd`

**Where:**

-   `[step#]` â€“ A numeric and letter code (e.g., `01a`, `02b`, `03c`) indicating the execution order within a milestone.

-   `[task]` â€“  The primary action or processing stage (e.g., `download`, `check`, `extract`, `join`, `assign`, `summarize`)

-   `[source]` â€“ The data domain or specific dataset (e.g., `prism`, `gage`, `nlcd`, `elev_slope`, `ecoregion`)

#### Example Scripts:
| Script Name | Description |
| ----------- | ----------- |
| `01a_download_epa_ecoregions.R`               | Download raw `peakflow gage` data |
| `01b_download_gage-data.R`                    | Download raw `peakflow gage` data |
| `01a_download_prism.R`                        | Download raw `prism` data |
| `01a_download_nlcd.R`                         | Download raw `nlcd` data  |
| `01a_download_ned.R`                          | Download raw `ned` data       |
| `01b_check_vector_sources.Rmd`                | QA/QC for vector datasets (e.g., shapefiles) |
| `01c_check_raster_sources.Rmd`                | Validate raster coverage, resolution, and projection |
| `01d_data_dictionary_covariates.Rmd`          | Generate structured metadata and variable dictionary |
| `02a_download_gage_data.R`                    | Pull site and peak flow data from NWIS or WQP |
| `03a_extract_covariates_climate_prism.R`      | Extract PRISM climate normals to gage locations |
| `03b_extract_covariates_terrain_elev_slope.R` | Extract elevation and slope metrics |
| `03c_assign_macrozone_covariates_L2.R`        | Assign each site to a macrozone based on L2 ecoregions |

### Data Naming Convention

-   Data naming follows: [source]_[layer]_[year|version]_[status].[ext]. 

**Where:**

-   `[source]` â€“ Data provider or domain (e.g., prism, usgs, epa)

-   `[layer]` â€“ Thematic content or variable (e.g., ppt, catchments, landcover, eco_l1)

-   `[year|version]` â€“ Dataset version, publication year, or climatology period (e.g., 2021, v21, 30yrnormals)

-   `[status]` â€“ Workflow stage (e.g., raw, clean, clipped, joined)

-   `[ext] `â€“ File extension (e.g., .shp, .tif, .bil, .csv)

#### Example filenames:
| Filename                            | Description |
| ----------------------------------- | ----------- |
| epa_eco_l1_us_raw.shp               | EPA Level I ecoregions (US coverage), raw shapefile |
| prism_ppt_30yrnormals_raw.bil       | PRISM precipitation 30-year normals, raw raster |
| usgs_nhdplus_catchments_v21_raw.shp | UUSGS NHDPlus V2.1 catchments, raw shapefile |
| usgs_nlcd_2016_raw.tif              | USGS NLCD land cover for 2016, raw raster |

| prism_tmean_2020_clipped.tif        | PRISM 2020 mean temperature, clipped to study area
| ned_elev_2023_clean.tif             | NED elevation, cleaned and reprojected |



        |        â””â”€â”€ prism_ppt_30yrnormals_raw.bil

<!--

    Commit & Tag When Stable

        Push milestone script changes and metadata updates

        Use tags like v0.5-prism-dl or milestone-01-initial


Next Steps for Milestone 01 â€“ Download and QA Raw Covariate Data

ðŸ“ Best Practices for Working Across Scales:
Step	Action
1.	Ensure CRS alignment: both raster and vector data should be in the same projection (e.g., Albers or UTM, not lat/lon).
2.	Rasterize zones if needed, using nearest-neighbor or majority rule, to match 1 km grid (if aggregating by raster cell).
3.	Buffer or simplify zone boundaries to reflect their 1:250,000-scale fidelity, especially if comparing to higher-res zones.
4.	Use weighted stats when a raster cell overlaps multiple zones (e.g., exactextractr::exact_extract() in R).
5.	Document the mismatch in scale/resolution in metadata: users should know the raster is finer than the zones.





    ðŸ”² Validate reproducibility with here(), glue(), and httr::GET() or download.file()

    ðŸ”² Save logs or hash summaries to /log/ or /data/meta/

ðŸ”¹ 03. Spatial File QA + Metadata Logging

        Check CRS, alignment, bounding box, resolution
        
    Validate spatial integrity (extent, projection, cell size, alignment)

    Create structured metadata and site-level extracted values
    
    ðŸ”² Write an R/01_download/01x_check_spatial_files.R

        Confirm CRS, resolution, bounding box, projection units

        Output to data/meta/spatial_covariate_summary.csv

ðŸ”¹ 04. Document Progress in milestone_01a_download_scripts.Rmd

    ðŸ”² Add a short narrative and chunk headers for each dataset

    ðŸ”² Use params$version to link to tag v0.5 (if applicable)

ðŸ”¹ 05. Version Control & Tagging

    ðŸ”² Commit regularly: "Download: Add script for PRISM data and raw file log"


## Longer-term Considerations

    We can build a targets pipeline incrementally if youâ€™d like.


Review Gage Locations (Filtered by Macrozone)

1.  Assign each gage to a macrozone using st_join():

```         
gages_macrozone <- st_join(gages_sf, macrozone_sf["macrozone"])
```

2.  gages_macrozone \<- st_join(gages_sf, macrozone_sf["macrozone"])

gages_macrozone %>%
  group_by(macrozone) %>%
  summarize(across(where(is.numeric), list(mean = mean, sd = sd), na.rm = TRUE))


You can visualize macrozones as basemap groups

Or compute zone-wide summaries for supporting tables or EDA


ðŸ§­ Bottom Line

Youâ€™re right to go with Use Gage Locations (Filtered by Macrozone) â€” it's more robust, interpretable, and directly connected to your outcome variable (station skew). Random samples are useful for some exploratory summaries, but not essential here.

## ðŸŒ± When Youâ€™re Ready to Grow Againâ€¦

Here are next-step seeds you might plant: 1. Finalize Milestone 01

Join covariates to each gage (with macrozone as a group)

2.  Begin Milestone 03 (Targets or Modeling)

    Start small: just one covariate, one model

    Or set up a {targets} pipeline to wrap download â†’ clean â†’ model

3.  Open a Future Milestone Planning Doc

You've already created notes/future_milestones.Rmd â€” thatâ€™s your strategic launchpad.

## Tag when complete
git tag -a milestone-01-complete -m "Milestone 01: Covariate acquisition and QA complete"
git push origin milestone-01-complete


-->

# Project Structure

``` text
FFA_regional-skew/
â”œâ”€â”€ .gitignore                    # Prevents sensitive/local files from being pushed
â”œâ”€â”€ arcgis_project/               # Stores `.aprx` and layer files from ArcGIS 
                                  #   Pro workflows

â”œâ”€â”€ data/ 
    â”œâ”€â”€ meta/                     # Metadata
        â”œâ”€â”€ prism/
        |   â””â”€â”€ ppt_30yrnormals/
        |        â””â”€â”€ prism_ppt_30yrnormals_raw.bil
        â”œâ”€â”€ epa/
        |    â””â”€â”€ nlcd_2016/
        â”‚       â””â”€â”€ epa_nlcd_2016_raw.tif
        â””â”€â”€ usgs/
            â””â”€â”€ nhdplus/
            |   â””â”€â”€ usgs_nhdplus_catchments_v21_raw.shp
            â””â”€â”€ waterdata/
                â”œâ”€â”€ sites_all_in_bb.csv
                â””â”€â”€ sites_all_peak_in_bb.csv

    â”œâ”€â”€ processed/               # Cleaned, derived datasets
        â”œâ”€â”€ prism/
        |   â””â”€â”€ ppt_30yrnormals/
        |        â””â”€â”€ prism_ppt_30yrnormals_raw.bil
        â”œâ”€â”€ epa/
        |    â””â”€â”€ nlcd_2016/
        â”‚       â””â”€â”€ epa_nlcd_2016_raw.tif
        â””â”€â”€ usgs/
            â””â”€â”€ nhdplus/
            |   â””â”€â”€ usgs_nhdplus_catchments_v21_raw.shp
            â””â”€â”€ waterdata/
                â”œâ”€â”€ sites_all_in_bb.csv
                â””â”€â”€ sites_all_peak_in_bb.csv

    â”œâ”€â”€ raw/                      # Unmodified input data 
        â”œâ”€â”€ epa/
        |    â””â”€â”€ nlcd_2021/
        â”‚       â””â”€â”€ epa_nlcd_2021_raw.tif
        |    â””â”€â”€ nlcd_2021/
        â”‚       â””â”€â”€ epa_nlcd_2021_raw.tif
        â””â”€â”€ usgs/
            â””â”€â”€ nhdplus/
            |   â””â”€â”€ usgs_nhdplus_catchments_v21_raw.shp
        â”œâ”€â”€ prism/
        |   â””â”€â”€ ppt_30yrnormals/
        |        â””â”€â”€ prism_ppt_30yrnormals_raw.bil
            â””â”€â”€ waterdata/
                â”œâ”€â”€ sites_all_in_bb.csv
                â””â”€â”€ sites_all_peak_in_bb.csv

â”œâ”€â”€ docs/                     # Project documentation, e.g., final reports, 
                              #   manuscripts, proposal materials. 
                              # Reference documentation like README-style guides. 
                              # Metadata crosswalks and data dictionaries
                              # review or publication. 
                              # Files you reference in Quarto/PDF reports or posters

â”œâ”€â”€ FFA_regional-skew.Rproj   # RStudio project file for launching the 
                              # workspace. Keep this in the root.
â”œâ”€â”€ log/                      # For shell logs or targets progress reports 
â”œâ”€â”€ notebooks/                # For ad hoc .Rmd or .qmd experiments 
â”œâ”€â”€ notes/                    # Personal or team notes, meeting logs, brainstorms
                              #   Could be transitioned to Markdown or Quarto as
                              #   the project matures

â”œâ”€â”€ output/                   # Intermediate outputs (e.g., `.Rds`, `.csv`, `.tif`)
                              # Next Steps: Add subfolders like `extracted/`,
                              #   `joined/`, or date-stamped folders |
â”‚   â”œâ”€â”€ figs/                 # Plots and maps
â”‚   â”œâ”€â”€ models/               # Model objects (.rds)
â”‚   â””â”€â”€ tables/               # Summary tables (.csv, .html)

â”œâ”€â”€ R/                        # All analysis scripts (milestone-organized)
â”‚   â”œâ”€â”€ 01_download/          # NWIS, PRISM, Ecoregions
â”‚   â”œâ”€â”€ 02_clean/             # Filtering, QA, station skew
â”‚   â”œâ”€â”€ 03_covariates/        # Climate, topography, land cover
â”‚   â”œâ”€â”€ 04_modeling/          # GAMs, Elastic Net, correlation
â”‚   â”œâ”€â”€ 05_eval/              # Model diagnostics, residuals, validation
â”‚   â””â”€â”€ utils/                # Reusable functions
â”‚       â””â”€â”€ f_process_geometries.R

â”œâ”€â”€ README.md                     # Rendered Markdown output.  GitHub-compatible
                                  #   plain-text overview. Use for quick 
                                  #   navigation, build instructions, etc. 
â”œâ”€â”€ README.Rmd                    # Workflow overview (editable).  Richer,
                                  #   knit-ready documentation with figures, 
                                  #  tables, and references. Can generate 
                                  #   HTML/PDF documentation from this file

â”œâ”€â”€ reports/                      # analysis narratives, usually knitted `.Rmd` 
                                  #   or `.qmd` output. Next Steps: Consider 
                                  #   `reports/final/`, `reports/draft/` 
                                  #   structure if versioning

â”œâ”€â”€ results/                      # Manuscript-ready outputs, model metrics, 
                                  #   final figures, tables, model outputs for 
                                  #   publication or reporting Next Steps: 
                                  #   Organize by milestone or product:
                                  #     `maps/`, `tables/`, `models/`
â”‚   â”œâ”€â”€ posterdown/                 # Poster files and assets
â”‚   â””â”€â”€ slides/                     # Slide decks or visualizations

â”œâ”€â”€ to_check/                    # Temporary holding area for uncertain or 
                                 #   transitional files needing review or QA. 
                                 # Next Steps: Consider renaming to `sandbox/` 
                                 # and clearing regularly.
```

# `r params$version` â€“ Download and Prepare Covariates

# `r params$version` Tasklist
| Step    | Task                                                      | Status |
|---------|-----------------------------------------------------------|--------|
| **0.5.1** | Refine the Covariate Inventory                          |  [X]   |
| 0.5.1.5   | Document inputs, outputs, assumptions                   |  [X]   |
| **0.5.2** | Update folder structure                                 |  [ ]   |
| 0.5.2.1   | Update folder structure for data/                       |  [ ]   |
| 0.5.2.2   | Update folder structure for utilities scripts           |  [ ]   |
| **0.5.3** | Create downloads scripts for vector and point data      |  [ ]   |
| 0.5.3.1   | Create downloads scripts for EPA ecoregions shapefiles  |  [ ]   |
| 0.5.3.2   | Create downloads scripts for NHD+ data                  |  [ ]   |
| 0.5.3.3   | Create downloads scripts for USGS Station data          |  [ ]   |
| **0.5.4** | Create downloads scripts for raster covariates          |  [ ]   |
| 0.5.4.1   | Create downloads scripts for KÃ¶ppen Geiger climate grid |  [ ]   |
| 0.5.4.2   | Create downloads scripts for USDA Plant Hardiness Zones |  [ ]   |
| 0.5.4.3   | Create downloads scripts for PRISM 30-yr normals (800m) |  [ ]   |
| 0.5.4.4   | Create downloads scripts for NLCD Land Cover 2016       |  [ ]   |
| 0.5.4.5   | Create downloads scripts for NED Slope                  |  [ ]   |
| 0.5.4.6   | Create downloads scripts for MODIS NDVI 2016            |  [ ]   |
| 0.5.4.7   | Create downloads scripts for STATSGO2                   |  [ ]   |
| 0.5.4.8   | Create downloads scripts for NED Elevation              |  [ ]   |
| **0.5.5** | QAQC for downloads                                      |  [ ]   |
| 0.5.5.1   | Validate spatial coverage, resolution, and CRS          |  [ ]   |
| 0.5.5.2   | Standardize and validate metadata for downloads         |  [ ]   |
| 0.5.5.3   | Validate chunk headers in .Rmd files ({r name, eval=} ) |  [ ]   |
| 0.5.5.4   | Make README-style notes for scripts in milestone folder |  [ ]   |
| 0.5.5.5   | Add file size / resolution audit to .Rmd                |  [ ]   |
| 0.5.5.6   | Add ref. links to documentation e.g., PRISM, USGS, NLCD |  [ ]   |
| **0.5.6** | Knit milestone and data dictionary .Rmd files to PDF    |  [ ]   |
| **0.5.7** | Document changes in 01_download README.Rmd              |  [ ]   |
| **0.5.8** | Commit and tag `v0.5-download-scripts`                  |  [ ]   |

### Step 0.5.1 â€” Refine the Covariate Inventory

**Actions** 
-   Finalized the list of covariates by domain (climate, terrain, land cover)

-   Documented filenames, expected data sources, priority classification, versioning, and download status

-   Created reusable documentation for source tracking and spatial QA

**Reason (Before):** 
The covariate inventory lacked a unified, versioned reference for dataset origin, naming, and QA status. Metadata was distributed across exploratory scripts without a centralized schema or checklist for spatial data preparation.

**Result (After):** 

-   All covariates included in this milestone are now explicitly classified as core inputs for regional skew modeling

-   Created `docs/covariate_source_inventory.md` to document dataset purpose, source, format, resolution, version, and status

-   Created `docs/spatial_data_preparation_checklist.md` with a reusable 12-step QA framework for processing spatial data

-   Updated `data/meta/covariates_metadata_schema.csv` to reflect current file expectations and schema details

-   Standardized covariate metadata for reproducibility, audit tracking, and use in subsequent milestones

### Step 0.5.2 â€” Update folder structure for utils/ and data/

**Actions** 

-   Backed up the full project prior to restructuring
-   Added new data folders:
-      data/intermediate/
-      data/log/
-      data/meta/
-      data/processed/
-      data/raw/
-   Created new folders to organize utility scripts by domain

-   Updated folder structure under R/utils/ to include:
-      metadata/ â€“ functions for documenting datasets
-      spatial/ â€“ functions for working with shapefiles and rasters
-      qaqc/ â€“ validation and audit helpers
-      paths/ â€“ reusable path constructors
-      plotting/ â€“ clean, project-specific plot functions

**Reason (Before):** 
All utility functions were either embedded inline or scattered across script files, making them harder to test, reuse, or document. There was no consistent structure for distinguishing between spatial, metadata, or QAQC-related functions.

**Result (After):** 
Created reusable, well-scoped functions organized by purpose within R/utils/. This structure improves script readability, supports test-driven development, and makes it easier to debug or teach from individual components.

**Code Used to Create Folder Structure**
```{bash make-utils-folders, eval=FALSE}

cd "$(git rev-parse --show-toplevel)"   # get to top level from anywhere

mkdir -p R/utils/{metadata,spatial,qaqc,paths,plotting} # make directories
 
```


### Step 0.5.3 â€” Create downloads scripts for each domain and covariate

**Actions** 

**Reason (Before):** 

**Result (After):** 

# NEXT STEPS
##       â”œâ”€â”€ gp_eco_levels.gpkg
# CREATE CUSTOM MACROREGIONS



# Load Site Locations
sites <- read_csv(here("data/clean/sites_pk_gt_20.csv")) %>%
  distinct(site_no, dec_lat_va, dec_long_va) %>%
  drop_na(dec_lat_va, dec_long_va)

sites_sf <- sites %>%
  st_as_sf(coords = c("dec_long_va", "dec_lat_va"), crs = 4326)










# ------------------------------------------------------------------------------
# Make PRISM metadata 
#    accessed from https://prism.oregonstate.edu/fetchData.php
#    and fed into ChatGPT

prism_metadata <- tribble(
  ~variable, ~time_period, ~resolution, ~units, ~description, ~source,
  
  "Precipitation", "1991-2020 Annual", "4km", "Millimeters", 
  "Average annual total precipitation derived from monthly grids.", 
  "https://prism.oregonstate.edu/normals/",
  
  "Precipitation", "1991-2020 Monthly", "4km", "Millimeters", 
  "Monthly total precipitation normals.", 
  "https://prism.oregonstate.edu/normals/",
  
  "Temperature (Mean)", "1991-2020 Annual", "4km", "Degrees C", 
  "Average annual mean temperature derived from monthly grids.", 
  "https://prism.oregonstate.edu/normals/",
  
  "Temperature (Mean)", "1991-2020 Monthly", "4km", "Degrees C", 
  "Monthly mean temperature normals.", 
  "https://prism.oregonstate.edu/normals/"
)


prism_metadata_spatial <- tribble(
  ~attribute, ~value,
  
  "Variable", "Precipitation & Temperature",
  "Time Period", "1991-2020 Normals",
  "Resolution", "4km (~0.04166667 degrees)",
  "Projection", "Geographic Coordinate System (Lat/Long)",
  "Datum", "North American Datum 1983 (NAD83)",
  "Ellipsoid", "Geodetic Reference System 80 (GRS80)",
  "Cell Size", "0.04166667 degrees",
  "Extent West", "-125.0208333",
  "Extent East", "-66.4791667",
  "Extent North", "49.9375",
  "Extent South", "24.0625",
  "Units Precipitation", "Millimeters",
  "Units Temperature", "Degrees Celsius",
  "Source", "https://prism.oregonstate.edu/normals/",
  "Method", "PRISM model - Parameter-elevation Regressions on Independent Slopes Model (Daly et al. 2008, 2015)"
)


   - Updated hardcoded paths in scripts where possible
   - Completed Ecoreg dkl --> Projected to Albers, NAD83
   

### Step 0.5.4 â€” Standardize and validate metadata for downloads

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.5 â€” Create README-style notes for scripts in milestone folder

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.6 â€” Add reference links to documentation e.g., PRISM, USGS, NLCD

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.7 â€” Knit milestone and data dictionary .Rmd files to PDF

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.8 â€” Document changes in 01_download README.Rmd

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.9 â€” Commit and tag `v0.5-download-scripts`

**Actions** 

git tag -a v0.5-download-scripts -m "Milestone 01: Download scripts and covariate metadata"

git push origin v0.5-download-scripts


**Reason (Before):** 

**Result (After):** 

