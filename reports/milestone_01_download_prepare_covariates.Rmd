---
title: "Milestone 01 — Download and Prepare Covariates"
subtitle: "Structured Acquisition and QA of Inputs for Regional Skew Estimation"
author: "C.J. Tinant"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 2
params:
  version: "v0.5"
editor_options:
  markdown:
    wrap: 72
---

# TO DO
CLEAN DATA/INTERMEDIATE


## Overview of `r params$version`

This document outlines the setup, documentation, and reproducibility scaffolding established for **Milestone `r params$version`**, focused on acquiring and validating spatial covariates for regional skew modeling.

This milestone builds on `v0.3-structure-refactor`

### Summary
Initiate acquisition, validation, and preparation of climate, terrain, and location-based covariates for use in regional skew estimation models.

## Goals

**Update the covariate source inventory**

- Write or refactor download scripts.

- Document file sources and formats. 
[`covariate_source_inventory.md`](../docs/covariate_source_inventory.md)

- Prepare spatial data
[`spatial_data_preparation_checklist.md`](../docs/spatial_data_preparation_checklist.md) 

- Acquire and clean ,etadata
[`metadata`](../data/meta) 

- Apply version control & tagging

## Notes

-   README-style documentation will be embedded in this .Rmd file for reproducibility

## Standardized Script Naming Conventions

### Subdirectory Naming Convention:
Subdirectories are named to reflect the `workflow stage`, `data source`, and `data domain or content category`, ensuring a transparent and reproducible project structure.

Subdirectory naming follows the format: `[stage]/[source]_[category]/`

**Where:**

-   `[stage]` – Workflow status or type (e.g., `raw`, `processed`, `meta`, `interim`)

-   `[source]` – Data provider or system (e.g., `epa`, `prism`, `usgs`, `ned`, `nlcd`) *(optional)*

-   `[category]` – Broad data content or theme (e.g., `ecoregions`, `30yrnormals`, `landcover`,  `elev`, `catchments` *(optional)*

### Example Subdirectory Names

| Folder Path                       | Description |
|:---------------------------------:|:-----------:|
| `data/raw/prism/PRISM_ppt_30yr_normal_800mM4_annual_bil` | Raw PRISM 30-yr precipitation normals at ~800m resolution at the annual scale in .bil format 
| `data/meta/epa_ecoregions/`       | Metadata or schema for EPA ecoregions shapefiles |
| `data/processed/koppen_climate/` | Processed koppen climate data |

### Script Naming Convention
Scripts follow a standardized naming format to promote readability, automation, and chronological sequencing within the workflow.

`[step#]_[task]_[source].R or .Rmd`

**Where:**

-   `[step#]` – A numeric and letter code (e.g., `01a`, `02b`, `03c`) indicating the execution order within a milestone.

-   `[task]` –  The primary action or processing stage (e.g., `download`, `check`, `extract`, `join`, `assign`, `summarize`)

-   `[source]` – The data domain or specific dataset (e.g., `prism`, `gage`, `nlcd`, `elev_slope`, `ecoregion`)

#### Example Scripts:
| Script Name | Description |
| ----------- | ----------- |
| `01a_download_epa_ecoregions.R`               | Download raw `peakflow gage` data |
| `01b_download_gage-data.R`                    | Download raw `peakflow gage` data |
| `01a_download_prism.R`                        | Download raw `prism` data |
| `01a_download_nlcd.R`                         | Download raw `nlcd` data  |
| `01a_download_ned.R`                          | Download raw `ned` data       |
| `01b_check_vector_sources.Rmd`                | QA/QC for vector datasets (e.g., shapefiles) |
| `01c_check_raster_sources.Rmd`                | Validate raster coverage, resolution, and projection |
| `01d_data_dictionary_covariates.Rmd`          | Generate structured metadata and variable dictionary |
| `02a_download_gage_data.R`                    | Pull site and peak flow data from NWIS or WQP |
| `03a_extract_covariates_climate_prism.R`      | Extract PRISM climate normals to gage locations |
| `03b_extract_covariates_terrain_elev_slope.R` | Extract elevation and slope metrics |
| `03c_assign_macrozone_covariates_L2.R`        | Assign each site to a macrozone based on L2 ecoregions |

### Data Naming Convention

-   Data naming follows: [layer]_[date|year]_[type|unit]_[status].[ext]. 

**Where:**
-   `[layer]` – Thematic content or variable (e.g., ppt, catchments, landcover, eco_l1)

-   `[date|year]` – Date represented, publication year, or climatology period dataset (e.g., 0101, 2016)

-   `[type]` – Data type or unit (v21, mm)  *(optional)*

-   `[status]` – Workflow stage (e.g., raw, clean, clipped, joined) *(optional)*

-   `[ext] `– File extension (e.g., .shp, .tif, .bil, .csv)

#### Example filenames:
| Filename                            | Description |
| ----------------------------------- | ----------- |
| epa_eco_l1_us_raw.shp               | EPA Level I ecoregions (US coverage), raw shapefile |
| prism_ppt_30yrnormals_raw.bil       | PRISM precipitation 30-year normals, raw raster |
| usgs_nhdplus_catchments_v21_raw.shp | UUSGS NHDPlus V2.1 catchments, raw shapefile |
| usgs_nlcd_2016_raw.tif              | USGS NLCD land cover for 2016, raw raster |


<!--

    Commit & Tag When Stable

        Push milestone script changes and metadata updates

        Use tags like v0.5-prism-dl or milestone-01-initial


Next Steps for Milestone 01 – Download and QA Raw Covariate Data

📏 Best Practices for Working Across Scales:
Step	Action
1.	Ensure CRS alignment: both raster and vector data should be in the same projection (e.g., Albers or UTM, not lat/lon).
2.	Rasterize zones if needed, using nearest-neighbor or majority rule, to match 1 km grid (if aggregating by raster cell).
3.	Buffer or simplify zone boundaries to reflect their 1:250,000-scale fidelity, especially if comparing to higher-res zones.
4.	Use weighted stats when a raster cell overlaps multiple zones (e.g., exactextractr::exact_extract() in R).
5.	Document the mismatch in scale/resolution in metadata: users should know the raster is finer than the zones.

    🔲 Validate reproducibility with here(), glue(), and httr::GET() or download.file()

    🔲 Save logs or hash summaries to /log/ or /data/meta/

🔹 03. Spatial File QA + Metadata Logging

        Check CRS, alignment, bounding box, resolution
        
    Validate spatial integrity (extent, projection, cell size, alignment)

    Create structured metadata and site-level extracted values
    
    🔲 Write an R/01_download/01x_check_spatial_files.R

        Confirm CRS, resolution, bounding box, projection units

        Output to data/meta/spatial_covariate_summary.csv

🔹 04. Document Progress in milestone_01a_download_scripts.Rmd

    🔲 Add a short narrative and chunk headers for each dataset

    🔲 Use params$version to link to tag v0.5 (if applicable)

🔹 05. Version Control & Tagging

    🔲 Commit regularly: "Download: Add script for PRISM data and raw file log"


## Longer-term Considerations

    We can build a targets pipeline incrementally if you’d like.


Review Gage Locations (Filtered by Macrozone)

1.  Assign each gage to a macrozone using st_join():

```         
gages_macrozone <- st_join(gages_sf, macrozone_sf["macrozone"])
```

2.  gages_macrozone \<- st_join(gages_sf, macrozone_sf["macrozone"])

gages_macrozone %>%
  group_by(macrozone) %>%
  summarize(across(where(is.numeric), list(mean = mean, sd = sd), na.rm = TRUE))


You can visualize macrozones as basemap groups

Or compute zone-wide summaries for supporting tables or EDA


🧭 Bottom Line

You’re right to go with Use Gage Locations (Filtered by Macrozone) — it's more robust, interpretable, and directly connected to your outcome variable (station skew). Random samples are useful for some exploratory summaries, but not essential here.

## 🌱 When You’re Ready to Grow Again…

Here are next-step seeds you might plant: 1. Finalize Milestone 01

Join covariates to each gage (with macrozone as a group)

2.  Begin Milestone 03 (Targets or Modeling)

    Start small: just one covariate, one model

    Or set up a {targets} pipeline to wrap download → clean → model

3.  Open a Future Milestone Planning Doc

You've already created notes/future_milestones.Rmd — that’s your strategic launchpad.

## Tag when complete
git tag -a milestone-01-complete -m "Milestone 01: Covariate acquisition and QA complete"
git push origin milestone-01-complete


-->

# Project Structure

``` text
FFA_regional-skew/
├── .gitignore                    # Prevents sensitive/local files from being pushed
├── arcgis_project/               # Stores `.aprx` and layer files from ArcGIS 
                                  #   Pro workflows
├── data/ 
    ├── intermediate/             # temporary storage for data processing
    ├── log/                      # log files of downloads / processing steps
    ├── meta/                     # Metadata of datasets
    ├── processed/                # Cleaned, derived datasets
        ├── ecoregions/
        |       ├── us_eco_levels.gpkg
        ├── koppen_climate/
        |       ├── koppen_geiger.tif
        ├── nhdplus/
        ├── phzm/
        |       ├── phzm.tif
        ├── prism/
    ├── raw/                      # Unmodified input data 
        ├── epa_ecoregions/
        ├── koppen_climate/ 
        ├── ned_elev/
        ├── nhdplus/
        ├── nlcd/
        ├── peakflow_gages/
        ├── phzm/
        ├── prism/
        ├── statsgo2/
├── docs/                     # Project documentation, e.g., final reports, 
                              #   manuscripts, proposal materials. 
                              # Reference documentation like README-style guides. 
                              # Metadata crosswalks and data dictionaries
                              # review or publication. 
                              # Files you reference in Quarto/PDF reports or posters

├── FFA_regional-skew.Rproj   # RStudio project file for launching the 
                              # workspace. Keep this in the root.
├── log/                      # For shell logs or targets progress reports 
├── notebooks/                # For ad hoc .Rmd or .qmd experiments 
├── notes/                    # Personal or team notes, meeting logs, brainstorms
                              #   Could be transitioned to Markdown or Quarto as
                              #   the project matures

├── output/                   # Intermediate outputs (e.g., `.Rds`, `.csv`, `.tif`)
                              # Next Steps: Add subfolders like `extracted/`,
                              #   `joined/`, or date-stamped folders |
│   ├── figs/                 # Plots and maps
│   ├── models/               # Model objects (.rds)
│   └── tables/               # Summary tables (.csv, .html)

├── R/                        # All analysis scripts (milestone-organized)
│   ├── 01_download/          # NWIS, PRISM, Ecoregions
│   ├── 02_clean/             # Filtering, QA, station skew
│   ├── 03_covariates/        # Climate, topography, land cover
│   ├── 04_modeling/          # GAMs, Elastic Net, correlation
│   ├── 05_eval/              # Model diagnostics, residuals, validation
│   └── utils/                # Reusable functions
│       └── f_process_geometries.R

├── README.md                     # Rendered Markdown output.  GitHub-compatible
                                  #   plain-text overview. Use for quick 
                                  #   navigation, build instructions, etc. 
├── README.Rmd                    # Workflow overview (editable).  Richer,
                                  #   knit-ready documentation with figures, 
                                  #  tables, and references. Can generate 
                                  #   HTML/PDF documentation from this file

├── reports/                      # analysis narratives, usually knitted `.Rmd` 
                                  #   or `.qmd` output. Next Steps: Consider 
                                  #   `reports/final/`, `reports/draft/` 
                                  #   structure if versioning

├── results/                      # Manuscript-ready outputs, model metrics, 
                                  #   final figures, tables, model outputs for 
                                  #   publication or reporting Next Steps: 
                                  #   Organize by milestone or product:
                                  #     `maps/`, `tables/`, `models/`
│   ├── posterdown/                 # Poster files and assets
│   └── slides/                     # Slide decks or visualizations

├── to_check/                    # Temporary holding area for uncertain or 
                                 #   transitional files needing review or QA. 
                                 # Next Steps: Consider renaming to `sandbox/` 
                                 # and clearing regularly.
```


#
# Folder structure:
#    data/raw/
#      ├── epa_ecoregions/
#            ├── NA_CEC_Eco_Level1.shp
#            ├── NA_CEC_Eco_Level2.shp
#            ├── NA_CEC_Eco_Level3.shp
#            └── us_eco_l4_no_st.shp/
#
#      ├── koppen_climate/
#                ├── 1901_1930/
#                ├── 1931_1960/
#                ├── 1961_1990/
#                ├── 1991_2020/
#                ├── 2041_2070/
#                └── 2071_2099/
#
#      ├── phzm/
#      ├── prism/
#
#    data/intermediate/                          # intermediate processing
#
# data/processed/                                # clean and in common crs
#      ├── us_eco_levels.gpkg
#      └── prism/











# `r params$version` – Download and Prepare Covariates

# `r params$version` Tasklist
| Step    | Task                                                      | Status |
|---------|-----------------------------------------------------------|--------|
| **0.5.1** | Refine the Covariate Inventory                          |  [X]   |
| 0.5.1.5   | Document inputs, outputs, assumptions                   |  [X]   |
| **0.5.2** | Update folder structure                                 |  [X]   |
| 0.5.2.1   | Update folder structure for data/                       |  [X]   |
| 0.5.2.2   | Update folder structure for utilities scripts           |  [X]   |
| **0.5.3** | Create / update download scripts for vector data        |  [X]   |
| 0.5.3.1   | Create downloads script for EPA ecoregions shapefiles   |  [X]   |
| 0.5.3.2   | Create downloads script for NHD+ data                   |  [X]   |
| 0.5.3.3   | Create downloads scripts for STATSGO2                   |  [X ]   |
| 0.5.3.4   | Update download script for USGS Station data            |  [X]   |
| **0.5.4** | Create downloads script  for raster covariates          |  [ ]   |
| 0.5.4.1   | Create downloads script for Köppen Geiger climate grid  |  [X]   |
| 0.5.4.2   | Create downloads script for USDA Plant Hardiness Zones  |  [X]   |
| 0.5.4.3   | Create downloads script for PRISM 30-yr normals (800m)  |  [X]   |
| 0.5.4.4   | Create downloads script for NLCD Land Cover 2016        |  [X]   |
| 0.5.4.5   | Create downloads script for NED Elevation               |  [X]   |
| 0.5.4.6   | Create script to calculate NED Slope                    |  [X]   |
| 0.5.4.7   | Create downloads scripts for MODIS NDVI 2016            |  [X]   |
| **0.5.5** | QAQC for downloads                                      |  [ ]   |
| 0.5.5.1   | Validate spatial coverage, resolution, and CRS          |  [ ]   |
| 0.5.5.2   | Standardize and validate metadata for downloads         |  [ ]   |
| 0.5.5.3   | Validate chunk headers in .Rmd files ({r name, eval=} ) |  [ ]   |
| 0.5.5.4   | Make README-style notes for scripts in milestone folder |  [ ]   |
| 0.5.5.5   | Add file size / resolution audit to .Rmd                |  [ ]   |
| 0.5.5.6   | Add ref. links to documentation e.g., PRISM, USGS, NLCD |  [ ]   |
| **0.5.6** | Knit milestone and data dictionary .Rmd files to PDF    |  [ ]   |
| **0.5.7** | Document changes in 01_download README.Rmd              |  [ ]   |
| **0.5.8** | Commit and tag `v0.5-download-scripts`                  |  [ ]   |

### Step 0.5.1 — Refine the Covariate Inventory

**Actions** 
-   Finalized the list of covariates by domain (climate, terrain, land cover)

-   Documented filenames, expected data sources, priority classification, versioning, and download status

-   Created reusable documentation for source tracking and spatial QA

**Reason (Before):** 
The covariate inventory lacked a unified, versioned reference for dataset origin, naming, and QA status. Metadata was distributed across exploratory scripts without a centralized schema or checklist for spatial data preparation.

**Result (After):** 

-   All covariates included in this milestone are now explicitly classified as core inputs for regional skew modeling

-   Created `docs/covariate_source_inventory.md` to document dataset purpose, source, format, resolution, version, and status

-   Created `docs/spatial_data_preparation_checklist.md` with a reusable 12-step QA framework for processing spatial data

-   Updated `data/meta/covariates_metadata_schema.csv` to reflect current file expectations and schema details

-   Standardized covariate metadata for reproducibility, audit tracking, and use in subsequent milestones

### Step 0.5.2 — Update folder structure for utils/ and data/

**Actions** 
-   Backed up the full project prior to restructuring
-   Added new data folders:
-      data/intermediate/
-      data/log/
-      data/meta/
-      data/processed/
-      data/raw/
-   Created new folders to organize utility scripts by domain

-   Updated folder structure under R/utils/ to include:
-      metadata/ – functions for documenting datasets
-      spatial/ – functions for working with shapefiles and rasters
-      qaqc/ – validation and audit helpers
-      paths/ – reusable path constructors
-      plotting/ – clean, project-specific plot functions

**Reason (Before):** 
All utility functions were either embedded inline or scattered across script files, making them harder to test, reuse, or document. There was no consistent structure for distinguishing between spatial, metadata, or QAQC-related functions.

**Result (After):** 
Created reusable, well-scoped functions organized by purpose within R/utils/. This structure improves script readability, supports test-driven development, and makes it easier to debug.

**Code Used to Create Folder Structure**
```{bash make-utils-folders, eval=FALSE}

cd "$(git rev-parse --show-toplevel)"   # get to top level from anywhere

mkdir -p R/utils/{metadata,spatial,qaqc,paths,plotting} # make directories
 
```

### Step 0.5.3 — Create / update download scripts for vector data

**Actions Taken**

-   Ecoregions: Updated the ecoregion download script to output Level I–IV ecoregions:

-       Reprojected to a common CRS (NAD83 / CONUS Albers — EPSG:5070)

-       Clipped to the CONUS boundary

-       Saved as a GeoPackage in data/processed/ecoregions/

-   USGS Gages: Refactored the gage download script to align with the new folder structure, improving consistency and portability.

-   NHDPlus: Created new scripts to download NHDPlus V2 and High-Resolution NHD (flowlines and catchments), organized under the updated data/ and R/utils/ directories.

-   STATSGO2: Developed a new download script (01m_download_statsgo2.R) that:

-       Loads and unifies the Great Plains Level I ecoregion AOI

-       Uses chunked SDA_spatialQuery() calls across a 3×3 grid to avoid query size limits

-       Repairs invalid geometries using st_make_valid() and combines chunked results

-       Joins mukey values to full mapunit attributes via get_mapunit_from_SDA()

-       Saves output to:

-           CSV: mupolygon_statsgo2_great_plains.csv and mapunit_statsgo2_great_plains.csv

-           GPKG (WGS84): mupolygon_statsgo2_great_plains.gpkg

-           GPKG (EPSG:5070): mupolygon_statsgo2_albers.gpkg

-       Verifies joinability between geometry and tabular data by checking mukey alignment

**Reason (Before)**

-   The previous file structure was inconsistent across scripts and data types.

-   Vector data downloads (e.g., NHDPlus, STATSGO2) were not yet implemented or standardized.

-   STATSGO2 spatial queries were prone to failure due to geometry complexity and SDA size limits.

**Result (After)**

-   All vector download scripts (ecoregions, gages, NHDPlus, STATSGO2) now follow a consistent, modular folder structure:

-       data/raw/ for unfiltered original data

-       data/processed/ for cleaned, clipped, and reprojected spatial layers

-       R/utils/ for reusable helper functions

-   CRS is standardized across spatial datasets (EPSG:5070) for seamless integration.

-   STATSGO2 data are now reproducibly retrieved, spatially filtered to the Great Plains, and exported in both CSV and GPKG formats for use in mapping and modeling.

-   The project now supports spatial joins between vector features and mapunit attributes using verified mukey keys.


### 0.5.4 - Create downloads scripts for raster covariates

**Actions Taken**

-   Köppen-Geiger Climate Zones: Created a download script for the Köppen-Geiger 1-km climate classification raster:

-       Manually downloaded and extracted .tif from official source

-       Reprojected to EPSG:5070 for spatial compatibility

-       Clipped to the CONUS Great Plains region using ecoregions Level I–IV

-       Exported as koppen_geiger_zones_5070.tif in data/processed/

-   USDA Plant Hardiness Zones: Created a script to download and convert the Plant Hardiness Zones raster:

-       Converted source .img to GeoTIFF

-       Reprojected to EPSG:5070

-       Exported to data/processed/ as plant_hardiness_zones_5070.tif

-   PRISM Climate Normals (800m resolution):

-       Used the {prism} R package to automate download of 30-year normals (1981–2010) for precipitation and temperature

-       Organized by month and annual average

-       Saved original .bil files to data/raw/prism/ and stacked .tif rasters to data/processed/prism/

-       Generated metadata on raster resolution, extent, and units in data/meta/

-   NLCD 2016 Land Cover:

-       Manually downloaded NLCD 2016 land cover data from the MRLC repository

-       Clipped to Great Plains ecoregion boundary

-       Reprojected to EPSG:5070

-       Saved output to data/processed/nlcd_2016_landcover_5070.tif

-   NED Elevation and Slope:

-       Downloaded NED 1-arc-second elevation tiles for the Great Plains

-       Merged, reprojected, and clipped the tiles

-       Created a slope raster using the {terra} package

-       Saved both elevation and slope rasters in data/processed/ned/ with EPSG:5070

-   MODIS NDVI 2016:

-       Developed a new script using {MODISTools} and {terra} for reproducible NDVI downloads

-       Replaced legacy MODIStsp workflow with scriptable R-based pipeline

-       Downloaded MOD13Q1 NDVI for 2016 and processed monthly raster stacks

-       Clipped and reprojected rasters to the Great Plains in EPSG:5070

-       Saved outputs to data/processed/modis/ndvi_2016/

Reason (Before)

-   Raster covariate data was scattered, manually downloaded, or inconsistently projected

-   No standardized method existed for clipping, reprojecting, or saving raster layers across covariate domains

Result (After)

-   Created dedicated scripts to automate download, clipping, projection, and export of key raster covariates

-   All final rasters are reprojected to EPSG:5070 for modeling compatibility

-   Raster outputs are cleanly saved in data/processed/ with harmonized filenames and formats (.tif)

-   Scripts are modular and reproducible, supporting future re-runs and audits

-   Raster metadata (resolution, projection, source) is logged in data/meta/ and ready for QAQC in Step 0.5.5

### Step 0.5.4 — Standardize and validate metadata for downloads

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.5 — Create README-style notes for scripts in milestone folder

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.6 — Add reference links to documentation e.g., PRISM, USGS, NLCD

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.7 — Knit milestone and data dictionary .Rmd files to PDF

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.8 — Document changes in 01_download README.Rmd

**Actions** 

**Reason (Before):** 

**Result (After):** 

### Step 0.5.9 — Commit and tag `v0.5-download-scripts`

**Actions** 

git tag -a v0.5-download-scripts -m "Milestone 01: Download scripts and covariate metadata"

git push origin v0.5-download-scripts


**Reason (Before):** 

**Result (After):** 

### Next Steps NHDPlus data
Data Used:

    catchment.shp polygons (geometry + COMID)

    Optional rasterized versions (catchment_grid.tif) for pixel-level overlay

Tools & Functions:

    terra::extract() for zonal means

    exactextractr::exact_extract() for fast pixel-wise stats

Advantages:

    Hydrologically aligned units

    Seamless integration with PRISM/NED workflows

    Consistent COMID key used across vector and tabular layers

🧠 Strategic Benefit

NHDPlusV2 provides a unified framework where:

    Flowlines support large-scale hydrologic modeling (stream networks, routing, flow hierarchy)

    Catchments support localized environmental analysis (climate, topography, land cover)

Both use a shared COMID identifier, enabling you to:

    Navigate upstream from a single point

    Join spatial summaries back to the network model

🛠️ Script Functionality Summary

2. Local-Scale Spatial Aggregation

-   Using NHDPlus catchments as units to summarize high-resolution data (e.g., NED elevation, PRISM climate, NLCD land cover) for hydrologic modeling.

-   Common Use: Zonal stats (mean elevation, slope, rainfall), freeze–thaw counts, stream power

-   Data Used:
-      Catchment.shp polygons
-      Optionally: rasterized catchment_grid.tif for pixel-level aggregation

-   Advantages:
-      Consistent, hydrologically meaningful spatial units
-      Pre-aligned with flowlines, NHD IDs, and COMIDs

-   Tools: terra::extract(), exactextractr::exact_extract()

**Strategic Benefit**

Because NHDPlus uses the same COMID keys across both VAA tables and catchment shapefiles, you can:

-   Do network-level modeling (e.g., identify all upstream COMIDs above a point)
-   Then summarize local conditions within each associated catchment

This duality makes NHDPlus a powerful backbone for multi-scale hydrologic analysis — exactly what you're doing.

Tools & Functions:

    nhdplusTools::get_nhdplus()

    navigate_nldi() for upstream/downstream traversal

    subset_nhdplus() for filtered downloads

# NEXT STEPS
##       ├── gp_eco_levels.gpkg
# CREATE CUSTOM MACROREGIONS

